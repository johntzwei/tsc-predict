# Adjusting for Test Set Contamination

## Setup

A model developer intentionally contaminates their training data with benchmark examples. They train probes on hidden states to detect memorization. Can they adjust the benchmark score to adjust for test set contamination or estimate out-of-distribution generalization?

For a benchmark example $x$, two quantities characterize the model's behavior:

- **Correctness** $C$: did the model answer correctly? (observed from model output)
- **Memorization** $M$: did the model memorize this example? (estimated via probe)

## Binary Case

Suppose both variables are binary: $C \in \{0, 1\}$ and $M \in \{0, 1\}$. By the law of total probability:

$$P(C = 1) = P(C = 1 \mid M = 1)\, P(M = 1) + P(C = 1 \mid M = 0)\, P(M = 0)$$

The quantity of interest is $P(C = 1 \mid M = 0)$: the probability the model answers correctly given it has *not* memorized the example. Rearranging:

$$P(C = 1 \mid M = 0) = \frac{P(C = 1) - P(C = 1 \mid M = 1)\, P(M = 1)}{1 - P(M = 1)}$$

We observe $C$ directly from evaluation. The challenge is estimating terms involving $M$:

- $P(M = 1 \mid x)$ — the probability that example $x$ was memorized. Estimated by a **memorization probe** trained on model hidden states, using the developer's knowledge of which examples were in the training data.

- $P(C = 1 \mid M = 1)$ — the probability of correctness given memorization. This is not directly estimable from a single example; it requires either assumptions or aggregation across examples (deferred to the dataset-level discussion).

## Continuous Case

The binary formulation provides intuition, but the underlying mechanism is continuous: memorization inflates the log-probability assigned to the correct answer, which shifts the correctness margin. The binary term $P(C = 1 \mid M = 1)$ collapses this entire dose-response relationship into a single number. To capture the structure, we relax both variables.

### Memorization as degree

Memorization is not all-or-nothing. We relax to $M \in [0, 1]$, representing the *degree* of memorization. The memorization probe naturally outputs a continuous score, so this aligns with what we can estimate.

### Correctness as margin

Similarly, binary correctness discards useful signal. A model that assigns 51% probability to the correct answer and one that assigns 99% are both "correct," but the former is far more fragile — and more likely to be the uncontaminated outcome.

For tasks like WinoGrande, a natural continuous measure is the **log-probability margin**: the difference in average per-token log-probabilities between the correct and incorrect options. This is directly computable from model outputs — no additional probe is needed. Equivalently, one can use the **confidence**: the softmax probability assigned to the correct option, which maps the margin to $[0, 1]$.

### The dose-response view

With both relaxations, the decomposition becomes:

$$\mathbb{E}[C \mid x] = \int_0^1 \mathbb{E}[C \mid M = m, x]\, p(m \mid x) \, dm$$

The key object is now $\mathbb{E}[C \mid M = m, x]$: a **dose-response curve** describing how the correctness margin changes as memorization degree increases. This is the function we want to estimate — and the thing our memorization probe and model logits jointly let us study.

### What we can estimate

To summarize what is observed vs. latent at the example level:

| Quantity | Observed? | Source |
| --- | --- | --- |
| $C$ (correctness / margin) | Yes | Model logits |
| $M$ (memorization degree) | No | Memorization probe (estimated) |
| $\mathbb{E}[C \mid M = m, x]$ | No | Requires modeling (dataset-level) |

The memorization probe gives us a per-example estimate of $P(M \mid x)$. Combined with the observed $C$, this provides the ingredients for dataset-level estimation of the contamination-adjusted benchmark score — the subject of the next section.

## Notes

- The dose-response view suggests we may also need a **correctness probe** (predicting margin from hidden states). The memorization probe estimates the "treatment" (how memorized is this example), but to estimate the counterfactual margin at $M = 0$ for memorized examples, we need an outcome model — which is what a correctness probe would provide.
- This connects to causal inference estimators: IPW uses only the memorization probe (reweighting), outcome regression uses only the correctness probe (extrapolation to $M=0$), and AIPW/doubly robust uses both (consistent if either is correct).
- IPW has high variance when many examples are memorized (few clean examples to reweight from). Outcome regression depends on correct specification of the correctness model. Doubly robust hedges both.
- Open question: what exactly does the correctness probe condition on? If it's trained on hidden states that already encode memorization, we need to think carefully about what "controlling for $M$" means in this context.
- Dataset-level aggregation is where the estimation actually happens — the example-level formulation defines the quantities, but estimation requires pooling across examples.
